{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis and Machine Learning Applications for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Material for a* [*University of Illinois*](http://illinois.edu) *course offered by the* [*Physics Department*](https://physics.illinois.edu). *This content is maintained on* [*GitHub*](https://github.com/illinois-mla) *and is distributed under a* [*BSD3 license*](https://opensource.org/licenses/BSD-3-Clause).\n",
    "\n",
    "[Table of contents](Contents.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mls import locate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_data = pd.read_hdf(locate_data('cluster_a_data.hf5'))\n",
    "b_data = pd.read_hdf(locate_data('cluster_b_data.hf5'))\n",
    "c_data = pd.read_hdf(locate_data('cluster_c_data.hf5'))\n",
    "d_data = pd.read_hdf(locate_data('cluster_d_data.hf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_3d = pd.read_hdf(locate_data('cluster_3d_data.hf5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo_data = pd.read_hdf(locate_data('cosmo_data.hf5'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciKit Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will be our first time using the [SciKit Learn package](http://scikit-learn.org/stable/).  We don't include it in our standard preamble since it contains many modules (sub-packages).  Instead, we import each module as we need it.  The ones we need now are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing, cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Structure in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of structure we can look for is \"clusters\" of \"nearby\" samples, but the definition of these terms requires some care.\n",
    "\n",
    "### Distance between samples\n",
    "\n",
    "In the simplest case, all features $x_{ij}$ have the same (possibly dimensionless) units, and the natural distance between samples (rows) $j$ and $k$ is:\n",
    "\n",
    "$$ \\Large\n",
    "d(j, k) = \\sum_{\\text{features}\\,i} (x_{ji} - x_{ki})^2 \\; .\n",
    "$$\n",
    "\n",
    "However, what if some columns have different units?  For example, what is the distance between:\n",
    "\n",
    "$$ \\Large\n",
    "\\left( 1.2, 0.4~\\text{cm}, 5.2~\\text{kPa}\\right)\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\Large\n",
    "\\left( 0.7, 0.5~\\text{cm}, 4.9~\\text{kPa}\\right)\n",
    "$$\n",
    "\n",
    "?\n",
    "\n",
    "ML algorithms are generally unit-agnostic, so will happily combine features with different units but that may not be what you really want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitening transformation\n",
    "\n",
    "One reasonable solution is to normalize each feature with the [whitening transformation](https://en.wikipedia.org/wiki/Whitening_transformation):\n",
    "\n",
    "$$ \\Large\n",
    "x \\rightarrow (x - \\mu) / \\sigma\n",
    "$$\n",
    "\n",
    "where $\\mu$, $\\sigma$ are the mean and standard deviation of the original feature values.\n",
    "\n",
    "It is called \"whitening\" because it transforms the input vector into a [white noise vector](https://en.wikipedia.org/wiki/White_noise)\n",
    "\n",
    "![WN1](img/Clustering/440px-White_noise.png)\n",
    "\n",
    "![WN2](img/Clustering/White-noise-mv255-240x180.png)\n",
    "\n",
    "Generally speaking, a whitening transformation is a linear transformation that transforms a vector of random variables with a known covariance matrix into a set of new variables whose covariance is the identity matrix, meaning that they are uncorrelated and each have unit variance. \n",
    "\n",
    "Suppose that $X$ is a column vector of random data with a non-singular covariance matrix $M$. Then the transformation \n",
    "\n",
    "$$ \\Large\n",
    "Y = WX\n",
    "$$\n",
    "\n",
    "with a whitening matrix $W$ satisfying the condition\n",
    "\n",
    "$$ \\Large\n",
    "W^T W = M^{-1}\n",
    "$$\n",
    "\n",
    "yields the whitened random vector $Y$ with unit diagonal convariance. There are an infinite nunber of possible whitening matrices that satisfy the condition above. One common choice is via the Principle Component Analyis (PCA) method which utilizes the eigen-system of $M$ to whiten $X$. We will come back to this in a bit when we talk more about dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why whiten the inputs?\n",
    "\n",
    "In general, learning algorithms benefit from standardization of the data set to minimize differences in the mean and variance of the input features. If some outliers are present in the set, robust scalers or transformers are more appropriate. We will learn more about this when we talk about the PCA method, but you can read [Importance of Feature Scaling](http://scikit-learn.org/stable/auto_examples/preprocessing/plot_scaling_importance.html#sphx-glr-auto-examples-preprocessing-plot-scaling-importance-py) if you want to read more now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [sklearn.preprocessing module](http://scikit-learn.org/stable/modules/preprocessing.html) automates this process with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo_normed = cosmo_data.copy()\n",
    "cosmo_normed[cosmo_data.columns] = preprocessing.scale(cosmo_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo_normed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this may discard useful information contained in the relative normalization between features. To normalize only certain columns use, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmo_normed = cosmo_data.copy()\n",
    "for colname in 'ln10^{10}A_s', 'H0':\n",
    "    cosmo_normed[colname] = preprocessing.scale(cosmo_data[colname])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a \"cluster\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest case (a), clusters are well separated by a line (in 2D, or hyperplane in more dimensions) and can be unambiguously identified by looking only at the distance between pairs of samples.\n",
    "\n",
    "In practice, clusters might overlap leading to ambiguities (b), or the clusters we expect to find might require considering groups of more than two samples at a time (c), or might have a non-linear separation boundary (d).\n",
    "\n",
    "![Cluster Types](img/Clustering/cluster_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Clustering in Physics\n",
    "\n",
    "The ability to cluster data from physics instruments is critical to being able to extract important features from the data on the road to making inference about nature. \n",
    "\n",
    "### Physics at the LHC\n",
    "\n",
    "There are many examples of data clustering in the study of particle collisions at the LHC. One important exampe is the clustering of data from calorimeter detectors to find the remants of quark or gluon production -- called \"jets\". A jet is a narrow cone of hadrons and other particles produced by the hadronization of a quark or gluon in a particle physics or heavy ion experiment. Particles carrying a color charge, such as quarks, cannot exist in free form because of QCD confinement which only allows for colorless states.\n",
    "\n",
    "![EventDisplay](img/Clustering/ATLAS_event_display_vp1_run266904_evt25855182_2015-06-03T13-41-48_b.png)\n",
    "\n",
    "![TwoJets](img/Clustering/2jets.jpg)\n",
    "\n",
    "### Astronomy\n",
    "\n",
    "Astrophysical objects have a variety of distinct objects and emissions that reflect the richness and beauty of our Universe. So, its not surprising that clustering algorthims to identify and ultimately understand astrophysical objects play a central role in Astronomy. \n",
    "\n",
    "As an example, astronomers use properties of Gamma Ray Bursts (GRBs) such as their location in the sky, arrival time, duration, fluence, spectral hardness to find subtypes/classes of events:\n",
    "\n",
    "![](img/Clustering/GRBs.png)\n",
    "\n",
    "![](img/Clustering/times.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [K-means algorithm](https://en.wikipedia.org/wiki/K-means_clustering) is fast and robust, but assumes that your data consists of roughly round clusters of the same size (where the meanings of \"round\" and \"size\" depend on how your data is scaled).\n",
    "\n",
    "Most sklearn algorithms use a similar calling pattern:\n",
    "```\n",
    "result = module.ClassName(..args..).fit(data)\n",
    "```\n",
    "For the [KMeans algorithm](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_fit = cluster.KMeans(n_clusters=2).fit(a_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the following function to display 2D cluster results (don't worry about the [details](https://seaborn.pydata.org/tutorial/color_palettes.html) unless you are interested):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(data, fit):\n",
    "    n_clusters = len(np.unique(fit.labels_))\n",
    "    # Pick good colors to distinguish the different clusters.\n",
    "    import matplotlib.colors\n",
    "    cmap = matplotlib.colors.ListedColormap(\n",
    "        sns.color_palette(\"husl\", n_clusters).as_hex())\n",
    "    plt.scatter(data.iloc[:, 0], data.iloc[:, 1], s=10, c=fit.labels_, cmap=cmap)\n",
    "    # Use standard axes to match the plot above.\n",
    "    plt.xlim(-9, +9)\n",
    "    plt.ylim(-5, +5)\n",
    "    plt.gca().set_aspect(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(a_data, a_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**EXERCISE:** Use KMeans to fit the three other (b,c,d) 2D datasets with `n_clusters=2` and generate similar plots. Which fits give the expected results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "b_fit = cluster.KMeans(n_clusters=2).fit(b_data)\n",
    "display(b_data, b_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "c_fit = cluster.KMeans(n_clusters=2).fit(c_data)\n",
    "display(c_data, c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "d_fit = cluster.KMeans(n_clusters=2).fit(d_data)\n",
    "display(d_data, d_fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "The fit results look reasonable for (b), although the sharp dividing line between the two clusters looks artificial.\n",
    "\n",
    "The fit results for (c) and (d) do not match what we expect because KMeans only considers one pair at a time, so cannot identify larger scale patterns that are obvious by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithms have many parameters that influence their results for a given dataset, but these fall into two categories:\n",
    "- Parameters whose values are determined by the data during the fitting process.\n",
    "- Parameters which must be externally set.\n",
    "\n",
    "We refer the second group as \"hyperparameters\" and set their values during the \"model selection\" process, which we will discuss later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "**DISCUSS:** Are all of the arguments of the [KMeans constructor](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) hyperparameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "In principle, yes, but in practice some of these arguments will have no (or minimal) impact on the algorithm result under normal conditions.  The arguments that are most clearly hyperparameters are:\n",
    "- `n_clusters`, `algorithm`, `tol`\n",
    "\n",
    "The arguments that are most clearly not hyperparameters are:\n",
    "- `verbose`, `precompute_distances`, `n_jobs`\n",
    "\n",
    "The remaining arugments are in the gray area.  In general, it is prudent to experiment with your actual data to identify which arguments affect your results significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**EXERCISE:** Fit dataset (b) with the `n_clusters` hyperparameter set to 3 and display the results. Comparing with the 2-cluster fit above, by eye, what do think is the \"true\" number of clusters?  How might you decide between 2 and 3 more objectively?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "b_fit_3 = cluster.KMeans(n_clusters=3).fit(b_data)\n",
    "display(b_data, b_fit_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The plot above makes a convincing case (to me, at least) that there are three clusters.  However, the \"truth\" in this case is two clusters.\n",
    "\n",
    "This illustrates the dangers of superimposing a fit result on your data: it inevitably \"draws your eye\" and makes the fit more credible. Look out for examples of this when reading papers or listening to talks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering in many dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm to find clusters in 2D data is just automating what you could already by eye.  However, most clustering algorithms also work well with higher dimensional data, where the clusters might not be visible in any single 2D projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_3d = cluster.KMeans(n_clusters=3).fit(cluster_3d)\n",
    "cluster_3d['label'] = fit_3d.labels_\n",
    "sns.pairplot(cluster_3d, vars=('x0', 'x1', 'x2'), hue='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These clusters look quite arbitrary in each of the 2D scatter plots. However, they are actually very well separated, as we can see if we rotate the axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = np.array(\n",
    "    [[ 0.5  , -0.5  , -0.707],\n",
    "     [ 0.707,  0.707,  0.   ],\n",
    "     [ 0.5  , -0.5  ,  0.707]])\n",
    "rotated_3d = cluster_3d.copy()\n",
    "rotated_3d[['x0', 'x1', 'x2']] = cluster_3d[['x0', 'x1', 'x2']].dot(R)\n",
    "sns.pairplot(rotated_3d, vars=('x0', 'x1', 'x2'), hue='label');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is contrived, but the lesson is that clustering algorithms can discover higher-dimensional structure that you might miss with visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General comments on ML Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have introduced our first ML algorithm, this is a good time for some general comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most ML algorithms have some common features:\n",
    "- They seek to maximize (or minimize) some goal function $f(\\theta, D)$ of the (fixed) data $D$, for some (unknown) parameters $\\theta$.\n",
    "- The goal function embodies some model (perhaps only implicitly) of what the data is expected to look like.\n",
    "\n",
    "Questions to ask about the goal function:\n",
    "- Is there a single global optimum by construction? (i.e., is $\\pm f$ a [convex function](https://en.wikipedia.org/wiki/Convex_function)?)\n",
    "- If not, might there be multiple local optima?\n",
    "\n",
    "Questions to ask about how the algorithm optimizes its goal function:\n",
    "- Is it exact or approximate?\n",
    "- If it is approximate, is it also iterative?  If so, what are the convergence criteria?\n",
    "- How does the running time scale with the number of samples and number of features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal function of the KMeans algorithm is:\n",
    "\n",
    "$$ \\Large\n",
    "\\sum_{i=1}^n\\, \\sum_{c_j = i}\\, \\left| x_j - \\mu_i\\right|^2\n",
    "$$\n",
    "\n",
    "where $c_j = 1$ if sample $j$ is assigned to cluster $i$ or otherwise $c_j = 0$, and\n",
    "\n",
    "$$ \\Large\n",
    "\\mu_i = \\sum_{c_j = i}\\, x_j\n",
    "$$\n",
    "\n",
    "is the mean of samples assigned to cluster $i$.  The outer sum is over the number of clusters $n$ and $j$ indexes samples. If we consider sample $x_j$ to be a vector, then its elements are the feature values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**DISCUSS:** What are the parameters of the KMeans goal function?  How many parameters are there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The parameters are the binary values $c_j$ and there is one per sample (row). Note that the number of parameters is independent of the number of features (columns) in the data.\n",
    "\n",
    "The number of clusters $n$ is a hyperparameter since it is externally set and not adjusted by the algorithm in response to the data.\n",
    "\n",
    "The means $\\mu_i$ are not independent parameters since their values are fixed by the $c_j$ (given the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised vs Unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML algorithms come in two flavors, depending on whether they require some training data where you already know the answer (\"supervised\") or not (\"unsupervised\"). Clustering algorithms are unsupervised.\n",
    "\n",
    "An advantage of unsupervised ML is that it works with any input data, and can discover patterns that you might not already know about (as in the 3D example above).  Even when you have training data available, an unsupervised algorithm can still be useful.\n",
    "\n",
    "The disadvantage of unsupervised learning is that we cannot formulate objective measures for how well an algorithm is performing, so the results are always somewhat subjective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KMeans algorithm uses an iterative solution based on the [Expectation-Maximization (EM)](https://en.wikipedia.org/wiki/Expectation-maximization_algorithm) principle. This is a powerful approach used by many algorithms, which we will revist several times during the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have focused on KMeans as a prototypical clustering algorithm, but there are [many others to chose from](http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html).\n",
    "\n",
    "We will finish this section with some brief experimentation with two alternatives that use more global information than KMeans, so are better suited to examples (c) and (d) above:\n",
    "- Spectral clustering: [sklearn](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html), [wikipedia](https://en.wikipedia.org/wiki/Spectral_clustering).\n",
    "- Density-based spatial clustering of applications with noise (DBSCAN): [sklearn](http://scikit-learn.org/stable/modules/clustering.html#spectral-clustering), [wikipedia](https://en.wikipedia.org/wiki/DBSCAN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**EXERCISE:** Use `cluster.SpectralClustering` to fit `c_data` and `d_data` and display the results.  Adjust the default hyperparameters, if necessary, to obtain the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "c_fit = cluster.SpectralClustering(n_clusters=2).fit(c_data)\n",
    "display(c_data, c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "d_fit = cluster.SpectralClustering(n_clusters=2, gamma=2.0).fit(d_data)\n",
    "display(d_data, d_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "**EXERCISE:** Use `cluster.DBSCAN` to fit `c_data` and `d_data` and display the results.  Adjust the default hyperparameters, if necessary, to obtain the expected results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "c_fit = cluster.DBSCAN(eps=1.5).fit(c_data)\n",
    "display(c_data, c_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "d_fit = cluster.DBSCAN().fit(d_data)\n",
    "display(d_data, d_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your solution here..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
