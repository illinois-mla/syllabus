{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning and Statistics for Physicists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.einsum` to evaluate the tensor expression $g^{il} \\Gamma^m_{ki} x^k$ which arises in [contravariant derivatives in General Relativity](https://en.wikipedia.org/wiki/Christoffel_symbols#Covariant_derivatives_of_tensors).  Note we are using the GR convention that repeated indices (k,l) are summed over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "67ebba3716136199857aaeefd0595675",
     "grade": false,
     "grade_id": "cell-b10c5a1cfe3128da",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def tensor_expr(g, Gamma, x, D=4):\n",
    "    \"\"\"Evaluate the tensor expression above.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    g : array\n",
    "        Numpy array of shape (D, D)\n",
    "    Gamma : array\n",
    "        Numpy array of shape (D, D, D)\n",
    "    x : array\n",
    "        Numpy array of shape (D,)\n",
    "    D : int\n",
    "        Dimension of input tensors.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        Numpy array of shape (D, D) that evaluates the tensor expression.\n",
    "    \"\"\"\n",
    "    assert g.shape == (D, D)\n",
    "    assert Gamma.shape == (D, D, D)\n",
    "    assert x.shape == (D,)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f91f3472db3a535b4aa76494682b0bbd",
     "grade": true,
     "grade_id": "cell-dc1412e0ed9e3c8f",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "g = np.arange(4 ** 2).reshape(4, 4)\n",
    "Gamma = np.arange(4 ** 3).reshape(4, 4, 4)\n",
    "x = np.arange(4)\n",
    "y = tensor_expr(g, Gamma, x)\n",
    "assert np.array_equal(\n",
    "    y,\n",
    "    [[ 1680,  3984,  6288,  8592], [ 1940,  4628,  7316, 10004],\n",
    "     [ 2200,  5272,  8344, 11416], [ 2460,  5916,  9372, 12828]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `np.histogram` to calculate the fraction of values in an arbitrary input data array that lie in each of the 10 intervals \\[0.0, 0.1), \\[0.1, 0.2), ..., \\[0.9, 1.0). You can assume that all input values are in the range \\[0,1). This is a useful technique to estimate the probability density that the data was sampled from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0af6aa7170b9f78496e1dbfd925016db",
     "grade": false,
     "grade_id": "cell-366b67031512bdaa",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def estimate_probability_density(data, bins):\n",
    "    \"\"\"Estimate the probability density of arbitrary data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : array\n",
    "        1D numpy array of random values.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    array\n",
    "        1D numpy array of N probability densities.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0)\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5029940f395172b2bbf68f048632eec",
     "grade": true,
     "grade_id": "cell-3add23f80d497553",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data = generator.uniform(size=100)\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho = estimate_probability_density(data, bins)\n",
    "assert np.allclose(0.1 * rho.sum(), 1.)\n",
    "assert np.allclose(rho, [ 0.6,  0.8,  0.7,  1.7,  1.1,  1.3,  1.6,  0.9,  0.8,  0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to calculate the [entropy](https://en.wikipedia.org/wiki/Entropy_estimation) $H(\\rho)$ of a binned probability density, defined as:\n",
    "$$\n",
    "H(\\rho) \\equiv -\\sum_i \\rho_i \\log(\\rho_i) \\Delta w_i \\; ,\n",
    "$$\n",
    "where $\\rho_i$ is the binned density in bin $i$ with width $w_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "cdacb2d2924ff83259b567883a4832d0",
     "grade": false,
     "grade_id": "cell-49d830408cabc403",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def binned_entropy(rho, bins):\n",
    "    \"\"\"Calculate the binned entropy.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : array\n",
    "        1D numpy array of densities, e.g., calculated by the previous function.\n",
    "    bins : array\n",
    "        1D numpy array of N+1 bin edges to use. Must be increasing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Value of the binned entropy.\n",
    "    \"\"\"\n",
    "    assert np.all(np.diff(bins) > 0)\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2de813a17b08d4d867107f8c4d1b1cee",
     "grade": true,
     "grade_id": "cell-7672bc6e182b3f89",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "generator = np.random.RandomState(seed=123)\n",
    "data1 = generator.uniform(size=10000)\n",
    "data2 = generator.uniform(size=10000) ** 4\n",
    "bins = np.linspace(0., 1., 11)\n",
    "rho1 = estimate_probability_density(data1, bins)\n",
    "rho2 = estimate_probability_density(data2, bins)\n",
    "H1 = binned_entropy(rho1, bins)\n",
    "H2 = binned_entropy(rho2, bins)\n",
    "assert np.allclose(H1, -0.000801544)\n",
    "assert np.allclose(H2, -0.699349908)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that reads `pong_data.hf5` and returns a new subset DataFrame containing only the columns `x5`, `y5`, `x7`, `y7` (**in that order**) and only the last 200 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "0e05585740687be735277fb545caa381",
     "grade": false,
     "grade_id": "cell-0cae4a532ac8ed42",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from mls import locate_data\n",
    "\n",
    "def create_subset():\n",
    "    \"\"\"Read pong_data.hf5 and return a subset.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "38f3af71e1904885fe03f432dd55f281",
     "grade": true,
     "grade_id": "cell-30143518143c64b1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# A correct solution should pass these tests.\n",
    "subset = create_subset()\n",
    "assert np.array_equal(subset.columns.values, ('x5', 'y5', 'x7', 'y7'))\n",
    "assert len(subset) == 200\n",
    "summary = subset.describe()\n",
    "assert np.allclose(summary.loc['mean', :].values,\n",
    "                   [ 0.43564752,  0.30610958,  0.57520991,  0.21383226])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
